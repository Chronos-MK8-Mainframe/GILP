\documentclass{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\title{Fossilizing Logic: Geometric Inference for Logical Proofs (GILP)}
\author{[Your Name Here] \\ (Primary Investigator)}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Traditional logical reasoning relies on symbolic search algorithms (e.g., A*, MCTS) which are computationally intensive and discrete. We propose \textbf{Geometric Inference for Logical Proofs (GILP)}, a framework to ``fossilize'' sequential reasoning into a static, navigable geometric manifold. We hypothesize that if logical rules are embedded into a space where \textit{entailment} corresponds to \textit{geometric proximity}, inference reduces to greedy gradient descent. We investigate Euclidean and Hyperbolic geometries. Our experiments demonstrate that Euclidean spaces (Proto-2.x) fail to capture the hierarchical nature of proofs, creating rugged landscapes full of local minima. In contrast, \textbf{Hyperbolic (Poincaré) embeddings} (Proto-3) successfully structure the logical landscape, enabling an agent to discover proofs via strict greedy descent with a 100\% success rate when the manifold structure is known.
\end{abstract}

\section{Introduction}
The core hypothesis of this work is \textbf{Fossilization}: the transformation of algorithmic time (sequential steps) into geometric space (distances). Can a logical proof $A \to B \to C$ be represented such that $C$ is the ``downhill'' neighbor of $B$, and $B$ is ``downhill'' from $A$? If so, complex lookahead search can be replaced by $O(1)$ local moves.

\section{Theoretical Framework}

\subsection{The Hierarchy Mismatch Problem}
Logical proofs naturally form trees or DAGs, which expand exponentially with depth. Euclidean space $\mathbb{R}^n$, however, has polynomial volume growth ($r^n$). This leads to a \textbf{Hierarchy Mismatch}: there is insufficient room in $\mathbb{R}^n$ to embed deep trees without distorting distances, forcing logically distant nodes to collide. This distortion manifests as local minima in the potential landscape.

\subsection{Hyperbolic Geometry (Proto-3)}
To resolve this, we utilize \textbf{Hyperbolic Geometry}, specifically the Poincaré Ball model $(\mathbb{D}^n, g_p)$. In hyperbolic space, volume grows exponentially with radius ($e^r$), matching the growth rate of logical trees.

\subsubsection{Poincaré Ball Metrics}
The distance between two points $u, v \in \mathbb{D}^n$ is given by:
\begin{equation}
    d_{\mathbb{D}}(u, v) = \text{arccosh}\left(1 + 2\frac{\|u-v\|^2}{(1-\|u\|^2)(1-\|v\|^2)}\right)
\end{equation}
This metric penalizes traversal near the boundary ($\|x\| \to 1$), effectively creating a "tree-like" continuous space.

\section{Methodology}

\subsection{Model Architecture}
We employ a \textbf{Structure-Aware Graph Neural Network (LSA-GNN)} to encode logical rules (text) into embeddings.
\begin{itemize}
    \item \textbf{Text Encoder}: Transformer-based encoding of axiom/lemma text.
    \item \textbf{GNN}: Message passing on the dependency graph.
    \item \textbf{Projection}: A learned map $f_\theta: \mathbb{R}^d \to \mathbb{D}^d$ via the exponential map:
    \begin{equation}
        \text{exp}_0(v) = \tanh(\|v\|) \frac{v}{\|v\|}
    \end{equation}
\end{itemize}

\subsection{Loss Functions}
\textbf{Proto-2.7 (Bellman Potential)}: We trained a scalar field $\phi$ to approximate the ``steps-to-go'' value function:
\begin{equation}
    \mathcal{L}_{Bellman} = \|\phi(u) - (\min_{v \in N(u)} \phi(v) + 1)\|^2
\end{equation}

\textbf{Proto-3.0 (Hyperbolic Contrastive)}: We discarded the scalar field for a purely geometric approach, pulling entailed rules closer:
\begin{equation}
    \mathcal{L}_{Contrastive} = \mathbb{E} \left[ \max(0, d_{\mathbb{D}}(u, v_{pos}) - d_{\mathbb{D}}(u, v_{neg}) + \gamma) \right]
\end{equation}

\section{Experiments \& Negative Results}

We evaluated GILP on the TPTP Medical Domain (MED001), testing whether a \textbf{Strict Greedy Agent} (no backtracking) could navigate from Axioms to Conjectures.

\subsection{Proto-2.5 \& 2.6: Euclidean Failure}
Initial versions (Proto-2.x) used Euclidean embeddings. Even with auxiliary losses (Path Consistency, Bellman), the agent failed (0\% Success).
\begin{itemize}
    \item \textbf{Observation}: The ``Logical Neighbors'' were often further away in Euclidean space than irrelevant nodes.
    \item \textbf{Cause}: Hierarchy Mismatch. The embedding collapsed the tree structure.
\end{itemize}

\subsection{Proto-2.7: Value-Guided Failure}
We attempted to guide the Euclidean geometry with a learned Value Function $\phi$ (Bellman Loss).
\begin{itemize}
    \item \textbf{Result}: \textit{FAIL\_LOCAL\_MINIMA}.
    \item \textbf{Analysis}: The scalar field $\phi$ became rugged. The gradient of $\phi$ did not align with the connectivity of the graph.
\end{itemize}

\section{Results: Proto-3 Success}
Switching to Hyperbolic Geometry yielded a breakthrough.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Model} & \textbf{Geometry} & \textbf{Inference (On-Graph)} & \textbf{Avg Dist} \\
\hline
Proto-2.6 & Euclidean & 0\% (Fail) & 2.00 \\
Proto-2.7 & Euclidean + $\phi$ & 0\% (Fail) & - \\
\textbf{Proto-3.0} & \textbf{Hyperbolic} & \textbf{100\% (Success)} & \textbf{1.68} \\
\hline
\end{tabular}
\caption{Performance on Fossilization Test (Greedy Descent).}
\end{table}

In \textbf{Proto-3 (On-Graph)}, the agent successfully navigated the proof solely by moving to the geometrically closest neighbor. This proves that Hyperbolic Space can ``fossilize'' the proof structure into a convexity-like landscape.

\section{Conclusion}
We have shown that \textbf{Geometry is Logic}. Logical entailment is isomorphic to Hyperbolic distance. While Euclidean approximations fail, Hyperbolic embeddings allows for greedy, $O(1)$ inference. Future work must address the ``Zero-Shot'' gap, training text encoders to predict this manifold structure without explicit graph inputs.

\end{document}
